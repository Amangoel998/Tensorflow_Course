# Word Embedding

- We can replace words with the key-value pair of the token which we generated by using tensorflow.
- Process strings to get indices of words in corpus of strings and convert strings into matrices of number.
- So we use string of numbers representing words.
- Embedding is process to extract features of a word from a corpus.
- Hence, words and associated words are clustered as vectors in multi-dimensional space.

What is the purpose of the embedding dimension? It is the number of dimensions for the vector representing the word
encoding

## Example

- In below image the reviews of a movies are clustered as positive & negative reviews in 3 dimensional space.
  <img src="word%20embedding.png" width="400">

## Tensorflow data services

- Has many datasets on lots of categories. Like imdb reviews datasets.
- Has 50,000 movie reviews which are classified as positive of negative.

To download tf datasets on colab.
`!pip install -q tensorflow-datasets`

```python
import tensorflow_datasets as tfds
import numpy as np

imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
# Data split into 25k samples each for training & testing
train_data, test_data = imdb['train'], imdb['test']

training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = []

for s, l in train_data:
    training_sentences.append(str(s.numpy()))
    training_labels.append(str(l.numpy()))

for s, l in test_data:
    testing_sentences.append(str(s.numpy()))
    testing_labels.append(str(l.numpy()))

training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)
```

- 1 in label indicates +ve and 0 as -ve.
- And labels are expected as numpy arrays.

```python
tf.Tensor(b'This is review', shape=(), dtype=string)
tf.Tensor(1, shape=(), dtype=int64)
```

- Then we will tokenize sentences.

```python
vocab_size = 10000
embedded_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = '<OOV>'

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequences import pad_sequences

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)

word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)

padded = pad_sequences(
    sequences,
    maxlen=max_length
truncating = trunc_type
)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences.maxlen = max_length)

```

# Code for Embedding

- The Enmedding layer will have shape like (10000, 16) -> (vocab_size, embeddings), having 16 dimensions.
- Due to size of vector being fed in (here, 120*16=1920), we use GlobalAveragePooling1D instead of Flatten.
- Reason, it averages across vectors to flatten it out. (model summary: (None, 16) instead of (None, 1920))

```python
model = tf.keras.Sequential([]
tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
# tf.keras.layers.Flatten(),
tf.keras.layers.GlobalAveragePooling1D(),
tf.keras.layers.Dense(6, activation='relu')
tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

num_epochs = 10
model.fit(padded,
training_labels_final,
epochs = num_epochs,
validation_data = (testing_padded, testing_labels_final))
```

## Plotting

```python
reverse_word_index = dict([(value, key) for key, value in word_index])

import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
    word = reverse_word_index[word_num]
    embeddings = weights[word_num]
    out_m.write(word + '\n')
    out_v.write('\t'.join([str(x) for x in embeddings]) + '\n')
out_v.close()
out_m.close()

# Download above files
from google.colab import files

files.download['vecs.tsv']
files.download['meta.tsv']
```

- Paste files into projector.tensorflow.org

## References

- https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%202%20-%20Lesson%201.ipynb#scrollTo=5NEpdhb8AxID
- https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%202%20-%20Lesson%202.ipynb